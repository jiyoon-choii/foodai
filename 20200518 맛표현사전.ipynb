{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/tripadvisor_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출, 원형 복원 \n",
    "# https://datascienceschool.net/view-notebook/8895b16a141749a9bb381007d52721c1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 단위 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도 시각화 \n",
    "# from nltk import Text \n",
    "# text = Text(retokenize.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We’d never had Korean before and I’d been want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I really was unsure of how much of the menu wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely delicious authentic Korean food ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The banchan or side dishes that they serve are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My eleven year old twins beg to get biminbop o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49335</th>\n",
       "      <td>Only 2 veggie dishes on menu - vegetable noodl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49336</th>\n",
       "      <td>We enjoyed ourselves so much we returned the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49337</th>\n",
       "      <td>Went with a couple of friends for dinner. Chic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49338</th>\n",
       "      <td>We both enjoyed our meal at this restaurant, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49339</th>\n",
       "      <td>The place was busy, doesn't take reservation a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49340 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "0      We’d never had Korean before and I’d been want...\n",
       "1      I really was unsure of how much of the menu wo...\n",
       "2      Absolutely delicious authentic Korean food ser...\n",
       "3      The banchan or side dishes that they serve are...\n",
       "4      My eleven year old twins beg to get biminbop o...\n",
       "...                                                  ...\n",
       "49335  Only 2 veggie dishes on menu - vegetable noodl...\n",
       "49336  We enjoyed ourselves so much we returned the n...\n",
       "49337  Went with a couple of friends for dinner. Chic...\n",
       "49338  We both enjoyed our meal at this restaurant, a...\n",
       "49339  The place was busy, doesn't take reservation a...\n",
       "\n",
       "[49340 rows x 1 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://foreverhappiness.me/35 - 불용어 처리 \n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-5%EC%9D%BC%EC%B0%A8-%EB%B6%88%EC%9A%A9%EC%96%B4-277694095a6f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    frequency_list = frequency.keys()\n",
    "    \n",
    "for words in frequency_list :\n",
    "#     print(words, frequency[words])\n",
    "    print(words, frequency[words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "result = []\n",
    "frequency = {}\n",
    "for review in df['review'][0:100] :\n",
    "    print(review)\n",
    "#     print(type(review))\n",
    "   \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     word_tokens = word_tokenize(reivew)\n",
    "#     pos_tag = pos_tag(word_tokenize(review))\n",
    "\n",
    "#     for w in pos_tag :\n",
    "#         if w not in stop_words :\n",
    "#             result.append(w)\n",
    "#             print(result)\n",
    "            \n",
    "# frequency_list = frequency.keys()    \n",
    "# for words in frequency_list :\n",
    "# #     print(words, frequency[words])\n",
    "#     print(words, frequency[words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 원핫인코더 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec의 하이퍼파라미터 \n",
    "      size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.  \n",
    "      window = 컨텍스트 윈도우 크기  \n",
    "      min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)  \n",
    "      workers = 학습을 위한 프로세스 수  \n",
    "      sg = 0는 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.wv.most_similar(\"delicious\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmqm(result[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "frequency = {}\n",
    "for word in result[0:100] :\n",
    "#     print(i)\n",
    "\n",
    "    match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', review)\n",
    "    # \\[ : [ \n",
    "    # \\b : 단어 구분자(Word boundary)\n",
    "    #{ , } : 결과를 input 값의 m에서 n 사이의 최대한 많은 반복과 일치하게 함 \n",
    "#     print(match_pattern)\n",
    "\n",
    "for word in match_pattern :\n",
    "#     print(word)\n",
    "    count = frequency.get(word,100)\n",
    "    frequency[word] = count + 1\n",
    "    \n",
    "#     frequency_list = frequency.keys()\n",
    "    \n",
    "# for words in frequency_list :\n",
    "# #     print(words, frequency[words])\n",
    "#     print(words, frequency[words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'again',\n",
       " 'all',\n",
       " 'also',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'because',\n",
       " 'blue',\n",
       " 'bother',\n",
       " 'box',\n",
       " 'busy',\n",
       " 'called',\n",
       " 'cashier',\n",
       " 'check',\n",
       " 'cheese',\n",
       " 'continue',\n",
       " 'decided',\n",
       " 'didn',\n",
       " 'dish',\n",
       " 'dishes',\n",
       " 'don',\n",
       " 'eat',\n",
       " 'egg',\n",
       " 'etc',\n",
       " 'exactly',\n",
       " 'expected',\n",
       " 'fast',\n",
       " 'food',\n",
       " 'for',\n",
       " 'fried',\n",
       " 'from',\n",
       " 'front',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'got',\n",
       " 'great',\n",
       " 'greeted',\n",
       " 'had',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hearing',\n",
       " 'highly',\n",
       " 'his',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'how',\n",
       " 'husband',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'instead',\n",
       " 'just',\n",
       " 'kimchi',\n",
       " 'later',\n",
       " 'less',\n",
       " 'like',\n",
       " 'little',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'lunch',\n",
       " 'menu',\n",
       " 'minutes',\n",
       " 'miso',\n",
       " 'never',\n",
       " 'next',\n",
       " 'one',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'others',\n",
       " 'our',\n",
       " 'party',\n",
       " 'pay',\n",
       " 'pieces',\n",
       " 'place',\n",
       " 'price',\n",
       " 'quickly',\n",
       " 'radish',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'recommend',\n",
       " 'relay',\n",
       " 'remember',\n",
       " 'return',\n",
       " 'rice',\n",
       " 'same',\n",
       " 'seat',\n",
       " 'seated',\n",
       " 'section',\n",
       " 'served',\n",
       " 'service',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'smelled',\n",
       " 'some',\n",
       " 'soup',\n",
       " 'stars',\n",
       " 'taking',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'this',\n",
       " 'time',\n",
       " 'tofu',\n",
       " 'top',\n",
       " 'try',\n",
       " 'very',\n",
       " 'waiter',\n",
       " 'walked',\n",
       " 'was',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'women']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 빈도수대로 stopword 지정 \n",
    "\n",
    "- 0:100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 0:100 \n",
    ": ['about','also', 'and','because','box','dish','dishes','etc','front','cashier', 'front', 'highly', 'his', 'husband', 'immediately','just','menu', 'minutes','others',\n",
    "'our','relay','section', 'some', 'that', 'the', 'their','them', 'then', 'this', 'very', 'walked', 'was', 'were','what', 'when', 'will', 'women' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 유의미 단어 \n",
    "- 0:100 \n",
    "['again', 'all', 'amazing', 'back', 'bad', 'blue', 'bother', 'busy', 'called',\n",
    " 'check', 'cheese', 'continue', 'decided', 'didn', 'don', 'eat', 'egg',, 'exactly',\n",
    " 'expected', 'fast', 'food', 'for', 'fried','from', 'get', 'getting', 'give','got',\n",
    " 'great','greeted', 'had', 'happy', 'hard', 'hearing', 'horrible', 'hot', 'how', \n",
    " 'impressed', 'instead', 'kimchi', 'later', 'less', 'like', 'little', 'looked',\n",
    " 'looking', 'lunch', 'miso', 'never', 'next', 'order', 'ordered', 'party', 'pay',\n",
    " 'pieces', 'place', 'price', 'quickly', 'radish', 'receive', 'received', 'recommend',\n",
    " 'remember', 'return', 'rice', 'same', 'seat', 'seated', 'served', 'service', 'sick',\n",
    " 'side', 'smelled', 'soup', 'stars', 'taking', 'than', 'time', 'tofu', 'top' 'try',\n",
    " 'waiter', 'with', 'without']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'again',\n",
       " 'all',\n",
       " 'also',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'because',\n",
       " 'blue',\n",
       " 'bother',\n",
       " 'box',\n",
       " 'busy',\n",
       " 'called',\n",
       " 'cashier',\n",
       " 'check',\n",
       " 'cheese',\n",
       " 'continue',\n",
       " 'decided',\n",
       " 'didn',\n",
       " 'dish',\n",
       " 'dishes',\n",
       " 'don',\n",
       " 'eat',\n",
       " 'egg',\n",
       " 'etc',\n",
       " 'exactly',\n",
       " 'expected',\n",
       " 'fast',\n",
       " 'food',\n",
       " 'for',\n",
       " 'fried',\n",
       " 'from',\n",
       " 'front',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'got',\n",
       " 'great',\n",
       " 'greeted',\n",
       " 'had',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hearing',\n",
       " 'highly',\n",
       " 'his',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'how',\n",
       " 'husband',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'instead',\n",
       " 'just',\n",
       " 'kimchi',\n",
       " 'later',\n",
       " 'less',\n",
       " 'like',\n",
       " 'little',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'lunch',\n",
       " 'menu',\n",
       " 'minutes',\n",
       " 'miso',\n",
       " 'never',\n",
       " 'next',\n",
       " 'one',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'others',\n",
       " 'our',\n",
       " 'party',\n",
       " 'pay',\n",
       " 'pieces',\n",
       " 'place',\n",
       " 'price',\n",
       " 'quickly',\n",
       " 'radish',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'recommend',\n",
       " 'relay',\n",
       " 'remember',\n",
       " 'return',\n",
       " 'rice',\n",
       " 'same',\n",
       " 'seat',\n",
       " 'seated',\n",
       " 'section',\n",
       " 'served',\n",
       " 'service',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'smelled',\n",
       " 'some',\n",
       " 'soup',\n",
       " 'stars',\n",
       " 'taking',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'this',\n",
       " 'time',\n",
       " 'tofu',\n",
       " 'top',\n",
       " 'try',\n",
       " 'very',\n",
       " 'waiter',\n",
       " 'walked',\n",
       " 'was',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'women']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처: https://3dmpengines.tistory.com/1647 [3DMP]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def modefinder(frequency_list):\n",
    "    c = Counter(frequency_list)\n",
    "    order = c.most_common()\n",
    "    maximum = order[5][1]\n",
    "#     print(maximum)\n",
    "\n",
    "    modes = []\n",
    "    for num in order:\n",
    "        print(num)\n",
    "#          if num[1] == maximum:\n",
    "#                 modes.append(num[0])\n",
    "#     return modes \n",
    "#     print(modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 단위 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('ve', 'RB'),\n",
       " ('been', 'VBN'),\n",
       " ('here', 'RB'),\n",
       " ('several', 'JJ'),\n",
       " ('times', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('m', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('fan', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('ve', 'JJ'),\n",
       " ('never', 'RB'),\n",
       " ('been', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('Korea', 'NNP'),\n",
       " ('so', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('m', 'RB'),\n",
       " ('far', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('expert', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Korean', 'JJ'),\n",
       " ('food', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('this', 'DT'),\n",
       " ('restaurant', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('clean', 'JJ'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('staff', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('friendly/professional', 'JJ'),\n",
       " (',', ','),\n",
       " ('there', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('many', 'JJ'),\n",
       " ('options', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('meals', 'NNS'),\n",
       " ('I', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('ve', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('excellent', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Give', 'VB'),\n",
       " ('I', 'PRP'),\n",
       " ('Love', 'NNP'),\n",
       " ('Korea', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('try', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('won', 'VBD'),\n",
       " ('’', 'VB'),\n",
       " ('t', 'NN'),\n",
       " ('be', 'VB'),\n",
       " ('disappointed', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('ll', 'JJ'),\n",
       " ('be', 'VB'),\n",
       " ('supporting', 'VBG'),\n",
       " ('local', 'JJ'),\n",
       " ('small', 'JJ'),\n",
       " ('business', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag \n",
    "sentence = df['review'][100]\n",
    "tagged_list = pos_tag(word_tokenize(sentence))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both were great.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(df['review'][0])[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터라이징 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in tqdm(tagged_list, desc='타이틀 리스트 진행도') :\n",
    "#     print(title)\n",
    "    cv = CountVectorizer()\n",
    "    corpus = []\n",
    "    \n",
    "    for doc_num in tqdm(range(5000), desc='문서진행도') :         \n",
    "        corpus.append(' '.join(title))\n",
    "#         print(corpus)\n",
    "\n",
    "    count_array = cv.fit_transform(corpus).toarray()\n",
    "    \n",
    "    feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We’d never had Korean before and I’d been wanting to try this for a while.',\n",
       " 'We let our server know we’d never had it before and she gave us some recommendations.',\n",
       " 'We ended up going with the beef bulgogi and the dolsot bibimbap.',\n",
       " 'Both were great.',\n",
       " 'It is important to know that some of the food is season to taste - so plan to add some of the table sauce and soy sauce to make it how you like it.',\n",
       " 'The appetizer sides that came with each entree were great too.',\n",
       " 'Service was excellent and everyone was very friendly.',\n",
       " 'Lots of regulars while we were there.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['review'][0]\n",
    "text_output = nltk.tokenize.sent_tokenize(text)\n",
    "text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-4d26e3d5e4e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(texts)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtext_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text_output: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "for i in df['review'] :\n",
    "    texts = word_tokenize(i)\n",
    "#     print(texts)\n",
    "    text_output = nltk.tokenize.sent_tokenize(texts)\n",
    "    print('text_output: {0}'.format(text_output))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-0acab5676713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\user\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m                 \u001b[1;31m# TODO speed up Series case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1238\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_from_nested_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "#dict 데이터프레임화 \n",
    "#ex = pd.DataFrame.from_dict(df['review'], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
